{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4339bb88",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification Algorithm-Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa771b",
   "metadata": {},
   "source": [
    "#### Classification in machine learning and statistics is a supervised learning approach in which the computer program learns from the data given to it and make new observations or classifications. Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as target, label or categories. The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which class/category the new data will fall into. Classification algorithms are used to predict the discrete output variables such as spam or not-spam, yes or no, heart disease or Not heart disease etc.\n",
    "\n",
    "#### Classification Algorithm we used in this data:\n",
    "- Logistic Regression\n",
    "- Naive Bayes Classifier\n",
    "- K-Nearest Neighbor\n",
    "- Support Vector Machine\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "- XGBoost\n",
    "- Artificial Neural Network(ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d28841",
   "metadata": {},
   "source": [
    "## Problem Analysis\n",
    "\n",
    "#### - We run all the classification models mention above with or without SMOTE, get best parameters, cross validate to check overfitting, use metrics(Accuracy, Precision, Recall and F-1) to check if model is good in unbalanced data.\n",
    "#### - This breast cancer dataset is from  UCI machine learning repository and was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. \n",
    "\n",
    "   ###  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "   10. Mitoses                       1 - 10\n",
    "   11. Class:                        (2 for benign, 4 for malignant)\n",
    "  \n",
    "#### - In this dataset we have 683 rows and 11 columns. The 10 independent variables learn the correlations with that dependent feature class variable predicting if the tumor is benign(2) or malignant(4). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff0132",
   "metadata": {
    "colab_type": "text",
    "id": "NvUGC8QQV6bV"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8870f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd753bf7",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2eef7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_BC = pd.read_csv('breast-cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4914d042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>776715</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>841769</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>888820</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "0               1000025                5                        1   \n",
       "1               1002945                5                        4   \n",
       "2               1015425                3                        1   \n",
       "3               1016277                6                        8   \n",
       "4               1017023                4                        1   \n",
       "..                  ...              ...                      ...   \n",
       "678              776715                3                        1   \n",
       "679              841769                2                        1   \n",
       "680              888820                5                       10   \n",
       "681              897471                4                        8   \n",
       "682              897471                4                        8   \n",
       "\n",
       "     Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "0                           1                  1                            2   \n",
       "1                           4                  5                            7   \n",
       "2                           1                  1                            2   \n",
       "3                           8                  1                            3   \n",
       "4                           1                  3                            2   \n",
       "..                        ...                ...                          ...   \n",
       "678                         1                  1                            3   \n",
       "679                         1                  1                            2   \n",
       "680                        10                  3                            7   \n",
       "681                         6                  4                            3   \n",
       "682                         8                  5                            4   \n",
       "\n",
       "     Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0              1                3                1        1      2  \n",
       "1             10                3                2        1      2  \n",
       "2              2                3                1        1      2  \n",
       "3              4                3                7        1      2  \n",
       "4              1                3                1        1      2  \n",
       "..           ...              ...              ...      ...    ...  \n",
       "678            2                1                1        1      2  \n",
       "679            1                1                1        1      2  \n",
       "680            3                8               10        2      4  \n",
       "681            4               10                6        1      4  \n",
       "682            5               10                4        1      4  \n",
       "\n",
       "[683 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9c7a2",
   "metadata": {},
   "source": [
    "#### 683 rows Ã— 11 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23694078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample code number             0\n",
       "Clump Thickness                0\n",
       "Uniformity of Cell Size        0\n",
       "Uniformity of Cell Shape       0\n",
       "Marginal Adhesion              0\n",
       "Single Epithelial Cell Size    0\n",
       "Bare Nuclei                    0\n",
       "Bland Chromatin                0\n",
       "Normal Nucleoli                0\n",
       "Mitoses                        0\n",
       "Class                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97073ae8",
   "metadata": {},
   "source": [
    "#### no null values noted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9abc80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 683 entries, 0 to 682\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype\n",
      "---  ------                       --------------  -----\n",
      " 0   Sample code number           683 non-null    int64\n",
      " 1   Clump Thickness              683 non-null    int64\n",
      " 2   Uniformity of Cell Size      683 non-null    int64\n",
      " 3   Uniformity of Cell Shape     683 non-null    int64\n",
      " 4   Marginal Adhesion            683 non-null    int64\n",
      " 5   Single Epithelial Cell Size  683 non-null    int64\n",
      " 6   Bare Nuclei                  683 non-null    int64\n",
      " 7   Bland Chromatin              683 non-null    int64\n",
      " 8   Normal Nucleoli              683 non-null    int64\n",
      " 9   Mitoses                      683 non-null    int64\n",
      " 10  Class                        683 non-null    int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 58.8 KB\n"
     ]
    }
   ],
   "source": [
    "dataset_BC.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69a29ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.830000e+02</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.076720e+06</td>\n",
       "      <td>4.442167</td>\n",
       "      <td>3.150805</td>\n",
       "      <td>3.215227</td>\n",
       "      <td>2.830161</td>\n",
       "      <td>3.234261</td>\n",
       "      <td>3.544656</td>\n",
       "      <td>3.445095</td>\n",
       "      <td>2.869693</td>\n",
       "      <td>1.603221</td>\n",
       "      <td>2.699854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.206440e+05</td>\n",
       "      <td>2.820761</td>\n",
       "      <td>3.065145</td>\n",
       "      <td>2.988581</td>\n",
       "      <td>2.864562</td>\n",
       "      <td>2.223085</td>\n",
       "      <td>3.643857</td>\n",
       "      <td>2.449697</td>\n",
       "      <td>3.052666</td>\n",
       "      <td>1.732674</td>\n",
       "      <td>0.954592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.337500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.776170e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.171795e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.238705e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.345435e+07</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "count        6.830000e+02       683.000000               683.000000   \n",
       "mean         1.076720e+06         4.442167                 3.150805   \n",
       "std          6.206440e+05         2.820761                 3.065145   \n",
       "min          6.337500e+04         1.000000                 1.000000   \n",
       "25%          8.776170e+05         2.000000                 1.000000   \n",
       "50%          1.171795e+06         4.000000                 1.000000   \n",
       "75%          1.238705e+06         6.000000                 5.000000   \n",
       "max          1.345435e+07        10.000000                10.000000   \n",
       "\n",
       "       Uniformity of Cell Shape  Marginal Adhesion  \\\n",
       "count                683.000000         683.000000   \n",
       "mean                   3.215227           2.830161   \n",
       "std                    2.988581           2.864562   \n",
       "min                    1.000000           1.000000   \n",
       "25%                    1.000000           1.000000   \n",
       "50%                    1.000000           1.000000   \n",
       "75%                    5.000000           4.000000   \n",
       "max                   10.000000          10.000000   \n",
       "\n",
       "       Single Epithelial Cell Size  Bare Nuclei  Bland Chromatin  \\\n",
       "count                   683.000000   683.000000       683.000000   \n",
       "mean                      3.234261     3.544656         3.445095   \n",
       "std                       2.223085     3.643857         2.449697   \n",
       "min                       1.000000     1.000000         1.000000   \n",
       "25%                       2.000000     1.000000         2.000000   \n",
       "50%                       2.000000     1.000000         3.000000   \n",
       "75%                       4.000000     6.000000         5.000000   \n",
       "max                      10.000000    10.000000        10.000000   \n",
       "\n",
       "       Normal Nucleoli     Mitoses       Class  \n",
       "count       683.000000  683.000000  683.000000  \n",
       "mean          2.869693    1.603221    2.699854  \n",
       "std           3.052666    1.732674    0.954592  \n",
       "min           1.000000    1.000000    2.000000  \n",
       "25%           1.000000    1.000000    2.000000  \n",
       "50%           1.000000    1.000000    2.000000  \n",
       "75%           4.000000    1.000000    4.000000  \n",
       "max          10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2091c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_BC['Class'] = dataset_BC['Class'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f3e890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "678    0\n",
       "679    0\n",
       "680    1\n",
       "681    1\n",
       "682    1\n",
       "Name: Class, Length: 683, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC['Class'] = dataset_BC['Class'].str.replace('2', '0').str.replace('4', '1')\n",
    "dataset_BC['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6f1d9",
   "metadata": {},
   "source": [
    "#### Change 2 to 0 for benign \n",
    "#### Change 4 to 1 for malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe66b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_BC['Class'] = dataset_BC['Class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7b00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample code number\n",
      "630\n",
      "Clump Thickness\n",
      "10\n",
      "Uniformity of Cell Size\n",
      "10\n",
      "Uniformity of Cell Shape\n",
      "10\n",
      "Marginal Adhesion\n",
      "10\n",
      "Single Epithelial Cell Size\n",
      "10\n",
      "Bare Nuclei\n",
      "10\n",
      "Bland Chromatin\n",
      "10\n",
      "Normal Nucleoli\n",
      "10\n",
      "Mitoses\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Check the datatype object unique number of values\n",
    "categorical = dataset_BC.select_dtypes(include='int64')\n",
    "for i in categorical:\n",
    "    column = categorical[i]\n",
    "    print(i)\n",
    "    print(column.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d49cc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba4cc10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    444\n",
       "1    239\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check classification label division\n",
    "dataset_BC['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f26f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop column that is not important \n",
    "dataset_BC.drop(['Sample code number'], 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02cec306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_BC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7d714",
   "metadata": {},
   "source": [
    "## Visualize the target to check division of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b7cbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catan\\.conda\\envs\\catanEnv\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dae3700348>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Univariate Analysis Target\n",
    "sns.countplot(dataset_BC['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349442c",
   "metadata": {},
   "source": [
    "#### we can see that the classification label division is unbalance, we have 444 benigh(0) and 239 malignant(1). We will not apply SMOTE to compare what the difference with SMOTE. We run in other jupyter notebook the data with SMOTE, result is listed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbabe0a",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fab74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset_BC.iloc[:, :-1].values\n",
    "y = dataset_BC.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac292ec8",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d17b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972a802",
   "metadata": {},
   "source": [
    "## 1. Training the Logistic Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc8a717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(random_state = 0)\n",
    "classifier_LR.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5754941",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946bf259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 3 47]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 94.00 %\n",
      "RECALL: 94.00 %\n",
      "F1-SCORE: 94.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "y_pred_LR = classifier_LR.predict(X_test_sc)\n",
    "cm_LR = confusion_matrix(y_test, y_pred_LR)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_LR)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_LR)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_LR)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_LR)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_LR)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a984425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Values</th>\n",
       "      <th>Real Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted Values  Real Values\n",
       "0                   0            0\n",
       "1                   0            0\n",
       "2                   1            1\n",
       "3                   1            1\n",
       "4                   0            0\n",
       "..                ...          ...\n",
       "132                 1            1\n",
       "133                 0            0\n",
       "134                 0            0\n",
       "135                 1            1\n",
       "136                 0            0\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_LR = pd.DataFrame({'Predicted Values': y_pred_LR.round(2), 'Real Values':  y_test})\n",
    "df_val_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0431c",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c67529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.98165138 0.98165138 0.96330275 0.95412844]\n",
      "AVERAGE ACCURACY: 96.71 %\n",
      "STANDARD DEVIATION: 1.24 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies_K_FOLD_LR = cross_val_score(estimator = classifier_LR, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies_K_FOLD_LR))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies_K_FOLD_LR.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies_K_FOLD_LR.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b041ab2",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16efdf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.07 %\n",
      "Best Parameters: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [0.001,0.01,0.1,1,10,100]}]\n",
    "grid_search = GridSearchCV(estimator = classifier_LR,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6af3a4",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 97.07 %, Best Parameters: {'C': 0.1}\n",
    "- W/O SMOTE Applied: CV=10, Best Accuracy: 96.88 %, Best Parameters: {'C': 0.1}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 97.04 %, Best Parameters: {'C': 0.01}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 97.07 %, Best Parameters: {'C': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342d8cb",
   "metadata": {},
   "source": [
    "#### After experimenting with SMOTE(balanced class),  W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed in the result above that there is a about 1% increase when class data is balance. The Logistic Regression can work well on the data even with or with out SMOTE applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c42bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(C = 0.1, random_state = 0)\n",
    "classifier_LR.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c4cbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 3 47]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 94.00 %\n",
      "RECALL: 94.00 %\n",
      "F1-SCORE: 94.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "y_pred_LR = classifier_LR.predict(X_test_sc)\n",
    "cm_LR = confusion_matrix(y_test, y_pred_LR)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_LR)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_LR)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_LR)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_LR)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_LR)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c951b",
   "metadata": {},
   "source": [
    "#### Accuracy is 95.62% that means that from the 137 tumors test set, 95.62%(131) is correctly classified and 6 is incorrectly classified. Of the 137 tumor test set sample, 87 is benigh and 50 is malignant. The model correctly identified 84 of the benign and identified correctly 47 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 94%, when it classifies a tumor to be malignant, it is correct 94% of the time.\n",
    "#### Recall is also 94%, It classifies 94% of the malignant tumors.\n",
    "#### F1-Score is also 94% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a useful metric only when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 94% the model perform well on this unbalanced data. This shows that Logistic Model is  a good model even though there is a class imbalanced data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17156357",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b1a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.99082569 0.98165138 0.96330275 0.96330275]\n",
      "AVERAGE ACCURACY: 97.07 %\n",
      "STANDARD DEVIATION: 1.34 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_LR, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894497ac",
   "metadata": {},
   "source": [
    "#### Average Accuracy increase from 96.71 % to 97.07 after applying best parameters.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that Linear Regression model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 95%, each average value is plus or minus 1.34 of the average accuracy of 97.07f%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d75a71",
   "metadata": {},
   "source": [
    "## 2. Training the Decision Tree Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb9551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_DT.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17658479",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c602d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 3 47]]\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 94.00 %\n",
      "RECALL: 94.00 %\n",
      "F1-SCORE: 94.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_DT = classifier_DT.predict(X_test_sc)\n",
    "cm_DT = confusion_matrix(y_test, y_pred_DT)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_DT)\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_DT)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_DT)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_DT)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_DT)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e96475",
   "metadata": {},
   "source": [
    "#### same result as from the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00340c97",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d213029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.90909091 0.94495413 0.94495413 0.91743119 0.93577982]\n",
      "AVERAGE ACCURACY: 93.04 %\n",
      "STANDARD DEVIATION: 1.47 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_DT, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3e35d",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8229a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 94.69 %\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'criterion': ['gini', 'entropy'], 'max_depth': [2,4,6,8,10,12]}]\n",
    "grid_search = GridSearchCV(estimator = classifier_DT,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fc748",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 94.69 % Best Parameters: {'criterion': 'gini', 'max_depth': 4}\n",
    "- W/O SMOTE Applied: CV=10, Best Accuracy: 94.87 % Best Parameters: {'criterion': 'gini', 'max_depth': 4}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 96.76 % Best Parameters: {'criterion': 'entropy', 'max_depth': 4}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 96.55 % Best Parameters: {'criterion': 'entropy', 'max_depth': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce46cf1",
   "metadata": {},
   "source": [
    "#### The Decision Tree Model also work well on the data even with or with out SMOTE applied. After experimenting with SMOTE(balanced class),  W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is only about 2% accuracy higher when SMOTE is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8636188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=4, random_state=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_DT = DecisionTreeClassifier(criterion = 'gini', max_depth= 4, random_state = 0)\n",
    "classifier_DT.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e4351",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c16374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[81  6]\n",
      " [ 2 48]]\n",
      "\n",
      "ACCURACY: 94.16 %\n",
      "PRECISION: 88.89 %\n",
      "RECALL: 96.00 %\n",
      "F1-SCORE: 92.31 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_DT = classifier_DT.predict(X_test_sc)\n",
    "cm_DT = confusion_matrix(y_test, y_pred_DT)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_DT)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_DT)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_DT)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_DT)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_DT)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315286a4",
   "metadata": {},
   "source": [
    "#### Accuracy is 94.16% that means that from the 137 tumors test set, 94.16%(128) is correctly classified and 8 is incorrectly classified. Of the 137 tumor test set sample, 83 is benigh and 54 is malignant. The model correctly identified 81 of the benign and identified correctly 48 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 88.89%, when it classifies a tumor to be malignant, it is correct 88.89% of the time.\n",
    "#### Recall is also 96%, It classifies 96% of the malignant tumors.\n",
    "#### F1-Score is also 92% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a goog metric when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 92.31 the Decision Tree Model perform little bit well on this unbalanced data. Precision is a little bit lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f0511f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.92727273 0.97247706 0.95412844 0.93577982 0.94495413]\n",
      "AVERAGE ACCURACY: 94.69 %\n",
      "STANDARD DEVIATION: 1.56 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_DT, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39836970",
   "metadata": {},
   "source": [
    "#### The accuracy increase from 93.04 % to 94.69 after applying best parameter.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that Decision Tree model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 92%, each average value is plus or minus 1.56 of the average accuracy of 94.69%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0dfc8",
   "metadata": {},
   "source": [
    "## 3. Training the K-NN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce88a1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier_KNN.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426879d3",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfd88f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[83  4]\n",
      " [ 2 48]]\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 92.31 %\n",
      "RECALL: 96.00 %\n",
      "F1-SCORE: 94.12 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_KNN = classifier_KNN.predict(X_test_sc)\n",
    "cm_KNN = confusion_matrix(y_test, y_pred_KNN)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_KNN)\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_KNN)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_KNN)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_KNN)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_KNN)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b92c1b",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a410daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.94545455 0.99082569 0.97247706 0.96330275 0.96330275]\n",
      "AVERAGE ACCURACY: 96.71 %\n",
      "STANDARD DEVIATION: 1.48 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_KNN, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1600dd",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ec95c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.44 %\n",
      "Best Parameters: {'leaf_size': 1, 'n_neighbors': 9, 'p': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'leaf_size': list(range(1,50)), 'n_neighbors': list(range(1,30)), 'p': [1,2]}]\n",
    "grid_search = GridSearchCV(estimator = classifier_KNN,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac4a9b",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, \n",
    "- W/O SMOTE Applied: CV=10, Best Accuracy: 97.44 % Best Parameters: {'leaf_size': 1, 'n_neighbors': 9, 'p': 2}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 98.31 % Best Parameters: {'leaf_size': 1, 'n_neighbors': 7, 'p': 2}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 98.20 % Best Parameters: {'leaf_size': 1, 'n_neighbors': 3, 'p': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c22ef",
   "metadata": {},
   "source": [
    "#### After experimenting KNN model with SMOTE(balanced class), W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is only about 1% average accuracy higher when SMOTE is applied.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6699a70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(leaf_size=1, n_neighbors=9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors = 9,leaf_size = 1, metric = 'minkowski', p = 2)\n",
    "classifier_KNN.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7099b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 3 47]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 94.00 %\n",
      "RECALL: 94.00 %\n",
      "F1-SCORE: 94.00 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_KNN = classifier_KNN.predict(X_test_sc)\n",
    "cm_KNN = confusion_matrix(y_test, y_pred_KNN)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_KNN)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_KNN)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_KNN)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_KNN)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_KNN)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b8231",
   "metadata": {},
   "source": [
    "#### We can see that nothing change in accuracy, precision, recall even though we applied best parameter. \n",
    "#### Accuracy is 95.62% that means that from the 137 tumors test set, 95.62%(131) is correctly classified and 6 is incorrectly classified. Of the 137 tumor test set sample, 87 is benigh and 50 is malignant. The model correctly identified 84 of the benign and identified correctly 47 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 94%, when it classifies a tumor to be malignant, it is correct 94% of the time.\n",
    "#### Recall is also 94%, It classifies 94% of the malignant tumors.\n",
    "#### F1-Score is also 94% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a good metric when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 94% the model perform well on this unbalanced data. This shows that KNN Model is  a good model on this data even though there is a class imbalanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12982d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.99082569 0.98165138 0.97247706 0.97247706]\n",
      "AVERAGE ACCURACY: 97.44 %\n",
      "STANDARD DEVIATION: 1.20 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_KNN, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa4a40",
   "metadata": {},
   "source": [
    "#### Average accuracy increase from 96.71 % from 97.44% after applying the best parameter.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that KNN model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 95%, each average value is plus or minus 1.20 of the average accuracy of 97.44%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41779641",
   "metadata": {},
   "source": [
    "## 4. Training the SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "312d6317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_SVM.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63dc41",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bc6f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[83  4]\n",
      " [ 2 48]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 92.31 %\n",
      "RECALL: 96.00 %\n",
      "F1-SCORE: 94.12 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_SVM = classifier_SVM.predict(X_test_sc)\n",
    "cm_SVM = confusion_matrix(y_test, y_pred_SVM)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_SVM)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_SVM)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_SVM)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_SVM)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_SVM)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c5d53",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d8f4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.98165138 0.97247706 0.97247706 0.97247706]\n",
      "AVERAGE ACCURACY: 97.07 %\n",
      "STANDARD DEVIATION: 0.88 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_SVM, X = X_train, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1976a",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac1d61",
   "metadata": {},
   "source": [
    "Best parameter is done in the Kernel SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb27de",
   "metadata": {},
   "source": [
    "## 5. Training the Kernel SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cc8f719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_KSVM = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_KSVM.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9a2ed",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47012ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[82  5]\n",
      " [ 1 49]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 90.74 %\n",
      "RECALL: 98.00 %\n",
      "F1-SCORE: 94.23 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_KSVM = classifier_KSVM.predict(X_test_sc)\n",
    "cm_KSVM = confusion_matrix(y_test, y_pred_KSVM)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_KSVM)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_KSVM)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd2d05",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3e2d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.94545455 0.98165138 0.98165138 0.97247706 0.97247706]\n",
      "AVERAGE ACCURACY: 97.07 %\n",
      "STANDARD DEVIATION: 1.33 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_KSVM, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e5357",
   "metadata": {
    "colab_type": "text",
    "id": "EHE5GWnf0hZJ"
   },
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94d7dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.07 %\n",
      "Best Parameters: {'C': 0.75, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},\n",
    "              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = classifier_KSVM,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562c737",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 97.07 % Best Parameters: {'C': 0.75, 'gamma': 0.1, 'kernel': 'rbf'\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 97.61 % Best Parameters: {'C': 1, 'gamma': 0.6, 'kernel': 'rbf'}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 97.27 % Best Parameters: {'C': 0.5, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e8ae6",
   "metadata": {},
   "source": [
    "#### The Kernel SVM Model also work well on the data even with or with out SMOTE applied. After experimenting with SMOTE(balanced class), W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is no change when SMOTE is applied..Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2133d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.75, gamma=0.1, random_state=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_KSVM = SVC(C=0.75, kernel = 'rbf', gamma =0.1, random_state = 0)\n",
    "classifier_KSVM.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72eb1184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[82  5]\n",
      " [ 1 49]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 90.74 %\n",
      "RECALL: 98.00 %\n",
      "F1-SCORE: 94.23 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_KSVM = classifier_KSVM.predict(X_test_sc)\n",
    "cm_KSVM = confusion_matrix(y_test, y_pred_KSVM)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_KSVM)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_KSVM)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_KSVM)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc210bc",
   "metadata": {},
   "source": [
    "#### Accuracy is 95.62% that means that from the 137 tumors test set, 95.62%(131) is correctly classified and 6 is incorrectly classified. Of the 137 tumor test set sample, 83 is benigh and 54 is malignant. The model correctly identified 82 of the benign and identified correctly 49 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 90.74%, when it classifies a tumor to be malignant, it is correct 90.74% of the time.\n",
    "#### Recall is also 98%, It classifies 98% of the malignant tumors.\n",
    "#### F1-Score is also 94.23% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a useful metric only when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 94.23% the model perform well on this unbalanced data. This shows that SVM Model is  a good model on this particular data  even though there is a class imbalanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc05eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.94545455 0.98165138 0.98165138 0.97247706 0.97247706]\n",
      "AVERAGE ACCURACY: 97.07 %\n",
      "STANDARD DEVIATION: 1.33 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_KSVM, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3156e2c",
   "metadata": {},
   "source": [
    "#### Average accuracy 97.07 didn't change  after applying the best parameter.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that SVM model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 94%, each average value is plus or minus 1.33 of the average accuracy of 97.07%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2995ca4",
   "metadata": {},
   "source": [
    "## 6. Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48a6529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407052af",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d194159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[80  7]\n",
      " [ 0 50]]\n",
      "\n",
      "ACCURACY: 94.89 %\n",
      "PRECISION: 87.72 %\n",
      "RECALL: 100.00 %\n",
      "F1-SCORE: 93.46 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_NB = classifier_NB.predict(X_test_sc)\n",
    "cm_NB = confusion_matrix(y_test, y_pred_NB)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_NB)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_NB)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_NB)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_NB)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_NB)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d60945",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3051f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.94545455 0.97247706 0.97247706 0.96330275 0.97247706]\n",
      "AVERAGE ACCURACY: 96.52 %\n",
      "STANDARD DEVIATION: 1.05 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_NB, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a8f4f",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "428eaacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 96.89 %\n",
      "Best Parameters: {'var_smoothing': 0.3359818286283782}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'var_smoothing': list(np.logspace(0,-9, num=20))}]\n",
    "grid_search = GridSearchCV(estimator = classifier_NB,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273f921",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 96.89 % Best Parameters: {'var_smoothing': 0.3359818286283782}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 96.62 % Best Parameters: {'var_smoothing': 0.012742749857031341}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 96.39 % Best Parameters: {'var_smoothing': 0.11288378916846892}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f1a81",
   "metadata": {},
   "source": [
    "#### The Naive Bayes model also work well on the data even with or with out SMOTE applied. After experimenting with SMOTE(balanced class), W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is little change on the accuracy when SMOTE is applied.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f27b2506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(var_smoothing=0.3359818286283782)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_NB = GaussianNB(var_smoothing = 0.3359818286283782)\n",
    "classifier_NB.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e36af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[83  4]\n",
      " [ 2 48]]\n",
      "\n",
      "ACCURACY: 95.62 %\n",
      "PRECISION: 92.31 %\n",
      "RECALL: 96.00 %\n",
      "F1-SCORE: 94.12 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_NB = classifier_NB.predict(X_test_sc)\n",
    "cm_NB = confusion_matrix(y_test, y_pred_NB)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_NB)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_NB)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_NB)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_NB)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_NB)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233451c",
   "metadata": {},
   "source": [
    "#### Accuracy is 95.62% that means that from the 137 tumors test set, 95.62%(131) is correctly classified and 6 is incorrectly classified. Of the 137 tumor test set sample, 85 is benigh and 52 is malignant. The model correctly identified 83 of the benign and identified correctly 48 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 92.31%, when it classifies a tumor to be malignant, it is correct 92.31% of the time.\n",
    "#### Recall is also 96%, It classifies 96% of the malignant tumors.\n",
    "#### F1-Score is also 94.12% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a useful metric when we have an equal distribution of classes on our classification. But base on the result of precision, recall and f1-score w/c is 94.12% the model perform well on this unbalanced data. This shows that Naive Bayes model is  a perform well on this data even though there is a class imbalanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21b1ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.98165138 0.98165138 0.97247706 0.95412844]\n",
      "AVERAGE ACCURACY: 96.89 %\n",
      "STANDARD DEVIATION: 1.23 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_NB, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba302ba0",
   "metadata": {},
   "source": [
    "#### Average Accuracy increase from 96.52% to 96.89% after applying the best parameter.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that Naive Bayes model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 95%, each average value is plus or minus 1.23 of the average accuracy of 96.89%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c970f86",
   "metadata": {},
   "source": [
    "## 7. Training the Random Forest Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "325dfa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier_RF.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4c3c9",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3444a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[83  4]\n",
      " [ 3 47]]\n",
      "\n",
      "ACCURACY: 94.89 %\n",
      "PRECISION: 92.16 %\n",
      "RECALL: 94.00 %\n",
      "F1-SCORE: 93.07 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_RF = classifier_RF.predict(X_test_sc)\n",
    "cm_RF = confusion_matrix(y_test, y_pred_RF)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_RF)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_RF)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_RF)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_RF)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_RF)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a48d2c",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9a4098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.94545455 0.98165138 0.98165138 0.97247706 0.97247706]\n",
      "AVERAGE ACCURACY: 97.07 %\n",
      "STANDARD DEVIATION: 1.33 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_RF, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347492c",
   "metadata": {},
   "source": [
    "## Applying Grid Search to find the best model and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a33a46f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.44 %\n",
      "Best Parameters: {'criterion': 'gini', 'max_features': 1, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'n_estimators': list(range(10,210,10)), 'criterion': ['gini', 'entropy'], 'max_features':list(range(1,6,1))}]\n",
    "grid_search = GridSearchCV(estimator = classifier_RF,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd30524c",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 97.44 % Best Parameters: {'criterion': 'gini', 'max_features': 1, 'n_estimators': 200}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 98.45 % Best Parameters: {'criterion': 'gini', 'max_features': 3, 'n_estimators': 140}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 98.05 % Best Parameters: {'criterion': 'gini', 'max_features': 1, 'n_estimators': 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869d82e",
   "metadata": {},
   "source": [
    "#### The Random Forest Classification model also work well on the data even with or with out SMOTE applied. After experimenting with SMOTE(balanced class), W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is only about 1% accuracy higher when SMOTE is applied.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7df577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=1, n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier(n_estimators = 200, criterion = 'gini', max_features= 1, random_state = 0)\n",
    "classifier_RF.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94eac59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 1 49]]\n",
      "\n",
      "ACCURACY: 97.08 %\n",
      "PRECISION: 94.23 %\n",
      "RECALL: 98.00 %\n",
      "F1-SCORE: 96.08 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred_RF = classifier_RF.predict(X_test_sc)\n",
    "cm_RF = confusion_matrix(y_test, y_pred_RF)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm_RF)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred_RF)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred_RF)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred_RF)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred_RF)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4277d7c",
   "metadata": {},
   "source": [
    "#### Accuracy is 97.08% that means that from the 137 tumors test set, 97.08%(133) is correctly classified and 4 is incorrectly classified. Of the 137 tumor test set sample, 85 is benigh and 52 is malignant. The model correctly identified 84 of the benign and identified correctly 49 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 94.23%, when it classifies a tumor to be malignant, it is correct 94.23% of the time.\n",
    "#### Recall is also 98%, It classifies 98% of the malignant tumors.\n",
    "#### F1-Score is also 96.08% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a useful metric when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 96.08% the model perform well on this unbalanced data. This shows that Random Forest Classification model is a good model even though there is a class imbalanced data set. It also lessen the FP(predicted as Benign but actually malignant) which is more important than FN(predicted as malignant but actually benigh) in medical diagnosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d843a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.99082569 0.97247706 0.96330275 0.99082569]\n",
      "AVERAGE ACCURACY: 97.44 %\n",
      "STANDARD DEVIATION: 1.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier_RF, X = X_train_sc, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f}\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f19874",
   "metadata": {},
   "source": [
    "#### Average Accuracy increase from 97.07 % to 97.44% after applying best parameters.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that Random Forest Classification model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 95%, each average value is plus or minus 1.46 of the average accuracy of 97.44%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd019dd",
   "metadata": {},
   "source": [
    "## Training XGBoost on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be836247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe7596",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25f297fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 97.08 %\n",
      "PRECISION: 94.23 %\n",
      "RECALL: 98.00 %\n",
      "F1-SCORE: 96.08 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea12df",
   "metadata": {},
   "source": [
    "## Applying k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f949ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.71 %\n",
      "Standard Deviation: 1.59 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} \".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "876a0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 97.44 %\n",
      "Best Parameters: {'eta': 0.0001, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{\"max_depth\": [1,2,3,4,5,6,7,8,9],\n",
    "         \"min_child_weight\": [1, 2, 3],\n",
    "         \"subsample\": [0.5,0.6,0.7,0.8,0.9, 1.0],\n",
    "         \"eta\": [1e-4,1e-3,1e-2,1e-1]}]\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train_sc, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed16c7",
   "metadata": {},
   "source": [
    "- W/O SMOTE Applied: CV=5, Best Accuracy: 97.44 % Best Parameters: {'eta': 0.0001, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8}\n",
    "- W/O SMOTE Applied: CV=10, Best Accuracy: 98.17 % Best Parameters: {'eta': 0.0001, 'max_depth': 2, 'min_child_weight': 2, 'subsample': 0.5}\n",
    "- WITH SMOTE: CV=5, Best Accuracy: 97.27 % Best Parameters: {'eta': 0.0001, 'eval_metric': 'logloss', 'max_depth': 1, 'min_child_weight': 3, 'subsample': 0.6}\n",
    "- WITH SMOTE: CV=10, Best Accuracy: 98.05 % Best Parameters: {'eta': 0.0001, 'max_depth': 2, 'min_child_weight': 3, 'subsample': 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647a378",
   "metadata": {},
   "source": [
    "#### The XGBoost Model also work well on the data even with or with out SMOTE applied. After experimenting with SMOTE(balanced class), W/O SMOTE(unbalanced data) and Cross-Validate it with 5 and 10 set of data, it is observed that there is only little difference less than 1% accuracy higher when SMOTE is applied.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a06c6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(eta=0.0001, max_depth=6, subsample=0.8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(eta = 0.0001, max_depth= 6, min_child_weight= 1, subsample= 0.8)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "118431a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[84  3]\n",
      " [ 1 49]]\n",
      "\n",
      "ACCURACY: 97.08 %\n",
      "PRECISION: 94.23 %\n",
      "RECALL: 98.00 %\n",
      "F1-SCORE: 96.08 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"CONFUSION MATRIX: \\n\", cm)\n",
    "print()\n",
    "print(\"ACCURACY: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"PRECISION: {:.2f} %\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"RECALL: {:.2f} %\".format(recall_score(y_test, y_pred)*100))\n",
    "print(\"F1-SCORE: {:.2f} %\".format(f1_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e697b9a",
   "metadata": {},
   "source": [
    "#### Accuracy is 97.08% that means that from the 137 tumors test set, 95.62%(133) is correctly classified and 4 is incorrectly classified. Of the 137 tumor test set sample, 85 is benigh and 52 is malignant. The model correctly identified 84 of the benign and identified correctly 49 of the malignant. Let us check the precision, recall and F1-Score if it work well with unbalanced data.\n",
    "#### Precision is 94.23%, when it classifies a tumor to be malignant, it is correct 94.23% of the time.\n",
    "#### Recall is also 98%, It classifies 98% of the malignant tumors.\n",
    "#### F1-Score is also 96.08% as it is the harmonic mean of the precision and Recall. \n",
    "#### Accuracy is a useful metric only when you have an equal distribution of classes on your classification. But base on the result of precision, recall and f1-score w/c is 96.08% the model perform well on this unbalanced data. This shows that XGBoost Model perform well on this dataset even though it has a class imbalanced data set. The same with Random Forest less FP and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0038c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION:  [0.95454545 0.99082569 0.97247706 0.96330275 0.99082569]\n",
      "AVERAGE ACCURACY: 97.44 %\n",
      "STANDARD DEVIATION: 1.46 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "print(\"CROSS VALIDATION: \", (accuracies))\n",
    "print(\"AVERAGE ACCURACY: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"STANDARD DEVIATION: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943883a",
   "metadata": {},
   "source": [
    "#### Average Accuracy increase from 97.08 % to 97.44% after applying best parameters.\n",
    "#### As shown above, the Cross Validation randomly split the data into 5 approximately equal size and accuracy in each set is calculated. We can see that XGBoost model perform well on each of the 5 divided dataset. The accuracy of each 5 set of data is near each other and above 95%, each average value is plus or minus 1.46 of the average accuracy of 97.44%. There is no overfitting noted, model overfits the data when it will give high accuracy on training data and low score on test set or unseen data. This helps us assess how well our model generalizes/perform on new datasets or unseen data and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a99386",
   "metadata": {},
   "source": [
    "#### After individually training the data to each of the classification models and tuning it, XGBoost Algorithm increase from 95.12 % to 97.23 %. We can conclude that the best model to used in this particular dataset is the XGBoost Algorithm. We should be careful also in parameter tuning because it might cause overfitting. But observed during the cross validation, there is no overfitting noted. We will train Deep Learning(Artificial Neural Network) to the powerplant data in the other jupyter notebook to see if ANN perform better than XGBRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2710c9b",
   "metadata": {},
   "source": [
    "### Classification Models Accuracy Result:\n",
    "\n",
    "#### XGB Classifier\n",
    "- CONFUSION MATRIX: \n",
    " - [[84  3]\n",
    " - [ 1 49]]\n",
    "- ACCURACY: 97.08 %\n",
    "- PRECISION: 94.23 %\n",
    "- RECALL: 98.00 %\n",
    "- F1-SCORE: 96.08 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 97.44 %\n",
    "\n",
    "#### Random Forest\n",
    "- CONFUSION MATRIX: \n",
    " - [[84  3]\n",
    " - [ 1 49]]\n",
    "- ACCURACY: 97.08 %\n",
    "- PRECISION: 94.23 %\n",
    "- RECALL: 98.00 %\n",
    "- F1-SCORE: 96.08 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 97.44 %\n",
    "\n",
    "#### K-Nearest Neighbor(K-NN)\n",
    "- CONFUSION MATRIX: \n",
    " - [[84  3]\n",
    " - [ 3 47]]\n",
    "- ACCURACY: 95.62 %\n",
    "- PRECISION: 94.00 %\n",
    "- RECALL: 94.00 %\n",
    "- F1-SCORE: 94.00 %\n",
    "\n",
    "- AVG ACCURACY: 97.44 %\n",
    "\n",
    "#### SVM_kernel\n",
    "- CONFUSION MATRIX: \n",
    " - [[82  5]\n",
    " - [ 1 49]]\n",
    "- ACCURACY: 95.62 %\n",
    "- PRECISION: 90.74 %\n",
    "- RECALL: 98.00 %\n",
    "- F1-SCORE: 94.23 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 97.07 %\n",
    "\n",
    "#### SVM\n",
    "- ACCURACY: 96.85 %\n",
    "- AVG CROS-VAL ACCURACY: 96.99 %\n",
    "\n",
    "#### Linear Regression\n",
    "- CONFUSION MATRIX: \n",
    " - [[84  3]\n",
    " - [ 3 47]]\n",
    "- ACCURACY: 95.62 %\n",
    "- PRECISION: 94.00 %\n",
    "- RECALL: 94.00 %\n",
    "- F1-SCORE: 94.00 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 96.71 %\n",
    "\n",
    "#### Decision Tree\n",
    "- CONFUSION MATRIX: \n",
    " - [[81  6]\n",
    " - [ 2 48]]\n",
    "- ACCURACY: 94.16 %\n",
    "- PRECISION: 88.89 %\n",
    "- RECALL: 96.00 %\n",
    "- F1-SCORE: 92.31 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 94.69 %\n",
    "\n",
    "#### Naive_Bayes\n",
    "- CONFUSION MATRIX: \n",
    " - [[83  4]\n",
    " - [ 2 48]]\n",
    "- ACCURACY: 95.62 %\n",
    "- PRECISION: 92.31 %\n",
    "- RECALL: 96.00 %\n",
    "- F1-SCORE: 94.12 %\n",
    "\n",
    "- AVG CROS-VAL ACCURACY: 96.24 %\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363532a9",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caaa523",
   "metadata": {},
   "source": [
    "#### - The result above is after individually training, tuning for best parameters, Cross Validate and apply SMOTE on this breast cancer data to each of the classification models. We can conclude that the best model for this particular data is the XGBoost Model and Random Forest Model. Both model have a highest average cross val of 97.44 and have lowest False Positive(FP) and False Negative(FN). This model has less  FP(predicted as Benign but actually malignant) which is more important than FN(predicted as malignant but actually benigh) in medical diagnosing. If we tell someone they don't have breast cancer when they actually do then they can go untreated as the cancer progresses, whereas telling someone they have cancer but actually they don't, more medical tests will be administered to confirm the diagnosis. We should be careful also in parameter tuning because it might cause overfitting. But observed during the cross validation, there is no overfitting noted.  \n",
    "#### - We will train Deep Learning(Artificial Neural Network) to this breast cancer data in the other jupyter notebook to see if ANN perform better than the Machine Learning Classification Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
